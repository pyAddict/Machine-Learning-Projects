{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train = pd.read_csv('./data/train.csv')\n",
    "test1 = pd.read_csv('./data/test1.csv')\n",
    "test2 = pd.read_csv('./data/test2.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# cnt = 0\n",
    "# for i in xrange( 0, train.review.size ):\n",
    "#     cnt+=1\n",
    "#     print train[\"review\"][i] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1729, 2) (179, 1) (76, 1)\n"
     ]
    }
   ],
   "source": [
    "print train.shape,test1.shape,test2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def review_to_words( raw_review ):\n",
    "    # Function to convert a raw review to a string of words\n",
    "    # The input is a single string (a raw movie review), and \n",
    "    # the output is a single string (a preprocessed movie review)\n",
    "    #\n",
    "    # 1. Remove HTML\n",
    "    review_text = BeautifulSoup(raw_review,'lxml').get_text() \n",
    "    #\n",
    "    # 2. Remove non-letters        \n",
    "    letters_only = re.sub(\"[^a-zA-Z]\", \" \", review_text) \n",
    "    #\n",
    "    # 3. Convert to lower case, split into individual words\n",
    "    words = letters_only.lower().split()                             \n",
    "    #\n",
    "    # 4. In Python, searching a set is much faster than searching\n",
    "    #   a list, so convert the stop words to a set\n",
    "    stops = set(stopwords.words(\"english\"))                  \n",
    "    # \n",
    "    # 5. Remove stop words\n",
    "    meaningful_words = [w for w in words if not w in stops]   \n",
    "    #\n",
    "    # 6. Join the words back into one string separated by space, \n",
    "    # and return the result.\n",
    "    return( \" \".join( meaningful_words ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaning and parsing the training set movie reviews...\n",
      "\n",
      "Review 100 of 1729\n",
      "\n",
      "Review 200 of 1729\n",
      "\n",
      "Review 300 of 1729\n",
      "\n",
      "Review 400 of 1729\n",
      "\n",
      "Review 500 of 1729\n",
      "\n",
      "Review 600 of 1729\n",
      "\n",
      "Review 700 of 1729\n",
      "\n",
      "Review 800 of 1729\n",
      "\n",
      "Review 900 of 1729\n",
      "\n",
      "Review 1000 of 1729\n",
      "\n",
      "Review 1100 of 1729\n",
      "\n",
      "Review 1200 of 1729\n",
      "\n",
      "Review 1300 of 1729\n",
      "\n",
      "Review 1400 of 1729\n",
      "\n",
      "Review 1500 of 1729\n",
      "\n",
      "Review 1600 of 1729\n",
      "\n",
      "Review 1700 of 1729\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Get the number of reviews based on the dataframe column size\n",
    "num_reviews = train[\"review\"].size\n",
    "\n",
    "print \"Cleaning and parsing the training set movie reviews...\\n\"\n",
    "clean_train_reviews = []\n",
    "for i in xrange( 0, num_reviews ):\n",
    "    # If the index is evenly divisible by 1000, print a message\n",
    "    if( (i+1)%100 == 0 ):\n",
    "        print \"Review %d of %d\\n\" % ( i+1, num_reviews )                                                                    \n",
    "    clean_train_reviews.append( review_to_words( train[\"review\"][i] ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating the bag of words...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print \"Creating the bag of words...\\n\"\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# Initialize the \"CountVectorizer\" object, which is scikit-learn's\n",
    "# bag of words tool.  \n",
    "vectorizer = CountVectorizer(analyzer = \"word\",   \\\n",
    "                             tokenizer = None,    \\\n",
    "                             preprocessor = None, \\\n",
    "                             stop_words = None,   \\\n",
    "                             max_features = 5000) \n",
    "\n",
    "# fit_transform() does two functions: First, it fits the model\n",
    "# and learns the vocabulary; second, it transforms our training data\n",
    "# into feature vectors. The input to fit_transform should be a list of \n",
    "# strings.\n",
    "train_data_features = vectorizer.fit_transform(clean_train_reviews)\n",
    "\n",
    "# Numpy arrays are easy to work with, so convert the result to an \n",
    "# array\n",
    "train_data_features = train_data_features.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1729, 4800)\n"
     ]
    }
   ],
   "source": [
    "print train_data_features.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training the random forest...\n"
     ]
    }
   ],
   "source": [
    "print \"Training the random forest...\"\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# Initialize a Random Forest classifier with 100 trees\n",
    "forest = RandomForestClassifier(n_estimators = 100) \n",
    "\n",
    "# Fit the forest to the training set, using the bag of words as \n",
    "# features and the sentiment labels as the response variable\n",
    "#\n",
    "# This may take a few minutes to run\n",
    "forest = forest.fit( train_data_features, train[\"label\"] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training the random forest...\n",
      "0.900554510015\n"
     ]
    }
   ],
   "source": [
    "from sklearn.cross_validation import KFold\n",
    "from sklearn.metrics import accuracy_score\n",
    "import numpy as np\n",
    "print \"Training the random forest...\"\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "forest = RandomForestClassifier(n_estimators = 100) \n",
    "n_folds = 10\n",
    "acc = []\n",
    "x = train_data_features\n",
    "y = np.array(train['label'])\n",
    "n = len(x)\n",
    "kf = KFold(n,n_folds,shuffle=False,random_state=None)\n",
    "for train_index,test_index in kf:\n",
    "    x_train,x_test = x[train_index],x[test_index]\n",
    "    y_train,y_test = y[train_index],y[test_index]\n",
    "    forest.fit(x_train,y_train)\n",
    "    y_pred = forest.predict(x_test)\n",
    "    acc.append(accuracy_score(y_test,y_pred))\n",
    "print np.mean(np.array(acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaning and parsing the test set movie reviews...\n",
      "\n",
      "Review 100 of 179\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Create an empty list and append the clean reviews one by one\n",
    "num_reviews = len(test1[\"review\"])\n",
    "clean_test_reviews = [] \n",
    "\n",
    "print \"Cleaning and parsing the test set movie reviews...\\n\"\n",
    "for i in xrange(0,num_reviews):\n",
    "    if( (i+1) % 100 == 0 ):\n",
    "        print \"Review %d of %d\\n\" % (i+1, num_reviews)\n",
    "    clean_review = review_to_words( test1[\"review\"][i] )\n",
    "    clean_test_reviews.append( clean_review )\n",
    "\n",
    "# Get a bag of words for the test set, and convert to a numpy array\n",
    "test_data_features = vectorizer.transform(clean_test_reviews)\n",
    "test_data_features = test_data_features.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Use the random forest to make sentiment label predictions\n",
    "result = forest.predict(test_data_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.  1.  1.  0.  0.  0.  0.  1.  0.  1.  1.  1.  1.  1.  1.  0.  1.  1.\n",
      "  1.  0.  1.  1.  0.  1.  1.  1.  0.  1.  0.  1.  0.  1.  1.  0.  1.  1.\n",
      "  1.  0.  0.  1.  1.  1.  1.  1.  0.  1.  1.  1.  1.  0.  0.  1.  1.  0.\n",
      "  0.  1.  0.  0.  1.  1.  1.  0.  1.  0.  1.  0.  1.  0.  1.  1.  1.  1.\n",
      "  1.  0.  1.  1.  1.  1.  1.  1.  0.  0.  1.  1.  1.  0.  1.  1.  1.  0.\n",
      "  1.  1.  0.  0.  1.  1.  0.  1.  1.  1.  1.  1.  1.  0.  1.  1.  1.  0.\n",
      "  1.  0.  1.  1.  1.  1.  1.  1.  0.  1.  0.  1.  1.  1.  1.  1.  1.  1.\n",
      "  1.  1.  1.  1.  1.  0.  1.  0.  1.  1.  1.  1.  0.  1.  1.  1.  0.  1.\n",
      "  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  0.  1.  1.  1.  1.  1.  1.  1.\n",
      "  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]\n"
     ]
    }
   ],
   "source": [
    "print result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score\n",
    "y_pred = [0, 2, 1, 3.0]\n",
    "y_true = [0, 1, 2, 3]\n",
    "accuracy_score(y_true, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
